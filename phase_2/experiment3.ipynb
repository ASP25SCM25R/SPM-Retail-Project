{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "622edc48",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY not found in environment. Please set it in your .env file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize LLM for LangChain\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize embedding model for LlamaIndex\u001b[39;00m\n\u001b[1;32m     22\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m OpenAIEmbedding(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:622\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m    619\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[1;32m    620\u001b[0m         )\n\u001b[1;32m    621\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/openai/_client.py:136\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m completions\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/openai/_base_client.py:825\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/openai/_base_client.py:755\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/httpx/_client.py:688\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, auth, params, headers, cookies, verify, cert, trust_env, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, default_encoding)\u001b[0m\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/httpx/_client.py:731\u001b[0m, in \u001b[0;36m_init_transport\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[0m\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/httpx/_transports/default.py:153\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[0m\n",
      "File \u001b[0;32m~/SPM-Retail-Project/lib/python3.10/site-packages/httpx/_config.py:40\u001b[0m, in \u001b[0;36mcreate_ssl_context\u001b[0;34m(verify, cert, trust_env)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:766\u001b[0m, in \u001b[0;36mcreate_default_context\u001b[0;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(purpose)\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[0;32m--> 766\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mverify_mode \u001b[38;5;241m!=\u001b[39m CERT_NONE:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs(purpose)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, Document  # Added Document here\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment. Please set it in your .env file.\")\n",
    "\n",
    "# Initialize LLM for LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "# Initialize embedding model for LlamaIndex\n",
    "embed_model = OpenAIEmbedding(api_key=api_key)\n",
    "\n",
    "# Directory paths\n",
    "index_dir = \"index_storage\"\n",
    "documents_dir = \"docs\"\n",
    "persist_file = os.path.join(index_dir, \"docstore.json\")\n",
    "\n",
    "# Ensure index directory exists\n",
    "if not os.path.exists(index_dir):\n",
    "    os.makedirs(index_dir)\n",
    "\n",
    "# Check if index already exists\n",
    "if not os.path.exists(persist_file):\n",
    "    if not os.path.exists(documents_dir):\n",
    "        os.makedirs(documents_dir)\n",
    "        raise FileNotFoundError(f\"No documents found in {documents_dir}. Please add documents to index.\")\n",
    "    \n",
    "    documents = SimpleDirectoryReader(documents_dir).load_data()\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    vector_index.storage_context.persist(persist_dir=index_dir)\n",
    "    print(f\"Created and persisted new index at {index_dir}\")\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_dir)\n",
    "    vector_index = load_index_from_storage(\n",
    "        storage_context,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    print(f\"Loaded existing index from {index_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine for retrieving context from the index\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dynamic prompt templates that use indexed data\n",
    "def get_agent_prompt(agent_name, input_var, input_data):\n",
    "    # Query the index for the agent's role and instructions\n",
    "    role_query = f\"What is the role and system message for the {agent_name} agent?\"\n",
    "    role_info = query_engine.query(role_query).response\n",
    "    \n",
    "    # Construct a dynamic prompt using the retrieved info and input data\n",
    "    prompt = f\"{role_info}\\n\\nBased on this role, process the following input:\\n{{{input_var}}}: {input_data}\"\n",
    "    return PromptTemplate(input_variables=[input_var], template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sprint function with index-driven prompts\n",
    "def run_sprint(customer_input, sprint_number):\n",
    "    print(f\"\\n--- Sprint {sprint_number} ---\")\n",
    "    \n",
    "    # Customer Proxy\n",
    "    customer_prompt = get_agent_prompt(\"Customer Proxy\", \"input\", customer_input)\n",
    "    customer_chain = LLMChain(llm=llm, prompt=customer_prompt, memory=ConversationBufferMemory())\n",
    "    requirements = customer_chain({\"input\": customer_input})[\"text\"]\n",
    "    vector_index.insert(Document(text=requirements, metadata={\"type\": \"requirements\", \"sprint\": sprint_number}))\n",
    "    print(\"Customer Proxy Output:\", requirements)\n",
    "    \n",
    "    # Product Owner\n",
    "    product_owner_prompt = get_agent_prompt(\"Product Owner\", \"requirements\", requirements)\n",
    "    product_owner_chain = LLMChain(llm=llm, prompt=product_owner_prompt, memory=ConversationBufferMemory())\n",
    "    backlog = product_owner_chain({\"requirements\": requirements})[\"text\"]\n",
    "    vector_index.insert(Document(text=backlog, metadata={\"type\": \"backlog\", \"sprint\": sprint_number}))\n",
    "    print(\"Product Owner Backlog:\", backlog)\n",
    "    \n",
    "    # Scrum Master\n",
    "    scrum_master_prompt = get_agent_prompt(\"Scrum Master\", \"backlog\", backlog)\n",
    "    scrum_master_chain = LLMChain(llm=llm, prompt=scrum_master_prompt, memory=ConversationBufferMemory())\n",
    "    sprint_plan = scrum_master_chain({\"backlog\": backlog})[\"text\"]\n",
    "    vector_index.insert(Document(text=sprint_plan, metadata={\"type\": \"sprint_plan\", \"sprint\": sprint_number}))\n",
    "    print(\"Scrum Master Sprint Plan:\", sprint_plan)\n",
    "    \n",
    "    # Development Team\n",
    "    dev_team_prompt = get_agent_prompt(\"Development Team\", \"sprint_plan\", sprint_plan)\n",
    "    dev_team_chain = LLMChain(llm=llm, prompt=dev_team_prompt, memory=ConversationBufferMemory())\n",
    "    deliverables = dev_team_chain({\"sprint_plan\": sprint_plan})[\"text\"]\n",
    "    vector_index.insert(Document(text=deliverables, metadata={\"type\": \"deliverables\", \"sprint\": sprint_number}))\n",
    "    print(\"Development Team Deliverables:\", deliverables)\n",
    "    \n",
    "    # Customer Feedback\n",
    "    feedback = input(f\"Review deliverables for Sprint {sprint_number}:\\n{deliverables}\\nEnter feedback: \")\n",
    "    vector_index.insert(Document(text=feedback, metadata={\"type\": \"feedback\", \"sprint\": sprint_number}))\n",
    "    print(\"Customer Feedback:\", feedback)\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 sprints\n",
    "initial_requirements = \"We need a system with real-time inventory tracking, automated reordering, and demand forecasting.\"\n",
    "feedback = initial_requirements\n",
    "for sprint in range(1, 4):\n",
    "    feedback = run_sprint(feedback, sprint)\n",
    "vector_index.storage_context.persist(persist_dir=index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query to verify the index\n",
    "response = query_engine.query(\"What was the feedback for Sprint 1?\")\n",
    "print(\"Query Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPM-Retail-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
